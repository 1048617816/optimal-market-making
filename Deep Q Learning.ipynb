{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "QzeQrzMu1EPM"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import math\n",
    "# make the simulation into an RL environment:\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "import matplotlib.pyplot as plt\n",
    "from runstats import *\n",
    "import runstats\n",
    "\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q2M5Wrp30Si_",
    "outputId": "e581e98b-3352-435a-9036-4b14a1de1ba4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matias/anaconda3/envs/GPU/lib/python3.7/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    }
   ],
   "source": [
    "# MS: Using a discrete action space similar to the Market Making via Reinforcement Learning paper (https://arxiv.org/pdf/1804.04216.pdf, Section 3 - p.3)\n",
    "actions_num = 21   #MS: So the range of possibilities goes from 0.3% to 3% from TOB\n",
    "max_abs_dif = 4\n",
    "max_abs_spread = 20\n",
    "\n",
    "\n",
    "s0 = 100\n",
    "T = 1. # Total time.\n",
    "sigma = 2.  # Standard deviation.\n",
    "dt = .005  # Time step.\n",
    "beta = 0.5\n",
    "kappa = beta * 2\n",
    "k = 1.5\n",
    "A = 137.45\n",
    "\n",
    "def spread(beta, sigma, T_t, k):\n",
    "    return beta*sigma**2*(T_t) + 2/beta*np.log(1+beta/k)\n",
    "\n",
    "def r(beta, sigma, T_t, s, q):\n",
    "    return s - q*beta*sigma**2*(T_t)\n",
    "\n",
    "def l(A, k, d):\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "      A : float\n",
    "        in Avellaneda A = \\lambda/\\alpha, where alpha is as above,\n",
    "        and lambda is the constant frequency of market buy and sell orders.\n",
    "      k : float\n",
    "        in Avellaneda k = alpha*K, where alpha ~ 1.5, \n",
    "        and K is such that \\delta p ~ Kln(Q) for a market order of size Q\n",
    "      d : float\n",
    "        in Avellaneda, d=distance to the mid price\n",
    "    \n",
    "    Return\n",
    "    -------\n",
    "    \n",
    "      l : float:\n",
    "        in Avellaneda, l = lambda = Poisson intensity at which our agentâ€™s orders are\n",
    "        executed.\n",
    "    '''\n",
    "    return A*np.exp(-k*d) \n",
    "    #JK: eq. (12)    \n",
    "\n",
    "\n",
    "class AvellanedaEnv:\n",
    "    def __init__(self, s0, T, dt, sigma, beta, k, A, kappa, seed=0, is_discrete=True):\n",
    "        '''\n",
    "        Parameters\n",
    "        ----------\n",
    "        s : float\n",
    "            Initial value of future/stock price.\n",
    "        b : float\n",
    "            Initial value of 'brecha'.\n",
    "        T : float\n",
    "            Total time.\n",
    "        dt : float\n",
    "            Time subdivision.\n",
    "        sigma : float\n",
    "            price volatility.\n",
    "        gamma : float\n",
    "            discount factor.\n",
    "        k : float\n",
    "            in Avellaneda k = alpha*K, where alpha ~ 1.5, \n",
    "            and K is such that \\delta p ~ Kln(Q) for a market order of size Q\n",
    "        A : float\n",
    "            in Avellaneda A = \\lambda/\\alpha, where alpha is as above,\n",
    "            and lambda is the constant frequency of market buy and sell orders.\n",
    "    \n",
    "        '''\n",
    "        self.s0 = s0\n",
    "        self.T = T\n",
    "        self.dt = dt\n",
    "        self.sigma = sigma\n",
    "        self.beta = beta\n",
    "        self.k = k\n",
    "        self.A = A\n",
    "        self.sqrtdt = np.sqrt(dt)\n",
    "        self.kappa = kappa\n",
    "        self.is_discrete = is_discrete\n",
    "        self.stats = runstats.ExponentialStatistics(decay=0.999)\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        # observation space: s (price), q, T-t (time remaining)\n",
    "        self.observation_space = gym.spaces.Box(low=np.array([0.0, -math.inf, 0.0]),\n",
    "                                     high=np.array([math.inf, math.inf,T]),\n",
    "                                     dtype=np.float32)\n",
    "        # action space: spread, ds\n",
    "        self.action_space = gym.spaces.Discrete(actions_num)\n",
    "        self.reward_range = (-math.inf,math.inf)\n",
    "        \n",
    "        self.metadata = None # useless field\n",
    "        \n",
    "    def reset(self,seed=0):\n",
    "        self.s = self.s0\n",
    "        self.q = 0.0\n",
    "        self.t = 0.0\n",
    "        self.w = 0.0\n",
    "        self.n = int(T/dt)\n",
    "        self.c_ = 0.0\n",
    "        return np.array((self.s,self.q,self.T))\n",
    "        \n",
    "    def step(self, action):\n",
    "        if self.is_discrete:\n",
    "            despl = (action-(actions_num-1)/2)*max_abs_dif/(actions_num-1)\n",
    "        else:\n",
    "            despl = action\n",
    "        ba_spread = spread(self.beta,self.sigma,self.T-self.t,self.k)\n",
    "\n",
    "        bid = self.s - despl - ba_spread/2\n",
    "        ask = self.s - despl + ba_spread/2\n",
    "                \n",
    "        db = self.s - bid\n",
    "        da = ask - self.s\n",
    "        \n",
    "        lb = l(A, k, db)\n",
    "        la = l(A, k, da)\n",
    "        \n",
    "        dnb = 1 if np.random.uniform() <= lb * self.dt else 0\n",
    "        dna = 1 if np.random.uniform() <= la * self.dt else 0\n",
    "        self.q += dnb - dna\n",
    "\n",
    "        self.c_ += -dnb * bid + dna * ask # cash\n",
    "\n",
    "        self.s += self.sigma * self.sqrtdt *(1 if np.random.uniform() < 0.5 else -1)\n",
    "\n",
    "        previous_w = self.w\n",
    "        self.w = self.c_ + self.q * self.s\n",
    "                \n",
    "        dw = (self.w - previous_w)\n",
    "        self.stats.push(dw)\n",
    "        #reward =  np.exp(-self.gamma*previous_w) - np.exp(-self.gamma*self.w) - 1/(self.n)\n",
    "        \n",
    "        #if self.t >= self.T:\n",
    "        reward = dw - self.kappa/2 * (dw - self.stats.mean())**2\n",
    "        \n",
    "        #if self.t >= self.T - self.dt:\n",
    "            #print(\"sum of dw: \" + str(sum(self.ws)))\n",
    "            #print(\"sum of kappa/2 * (dw - mu)**2: \" + str(sum(self.rews)))\n",
    "        \n",
    "        self.t += self.dt\n",
    "\n",
    "            \n",
    "        return np.array((self.s,self.q,self.T-self.t)), reward, self.t >= self.T, {'w':self.w}\n",
    "    \n",
    "env = AvellanedaEnv(s0, T, dt, sigma, beta, k, A,kappa)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6007 (pid 480899), started 0:10:16 ago. (Use '!kill 480899' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-8e027dcb7336fd36\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-8e027dcb7336fd36\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6007;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the TensorBoard notebook extension\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 434
    },
    "id": "ohfqOsj10psv",
    "outputId": "650c7370-1b20-42d5-c23f-4fb073ae6e12",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model not found! Starting training...\n",
      "Using cuda device\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to ./logs/DQN_2\n",
      "Eval num_timesteps=500, episode_reward=-7735.71 +/- 786.33\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.74e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.957     |\n",
      "| time/               |           |\n",
      "|    episodes         | 4         |\n",
      "|    fps              | 620       |\n",
      "|    time_elapsed     | 1         |\n",
      "|    total timesteps  | 900       |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=1000, episode_reward=-6760.36 +/- 990.04\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=1500, episode_reward=-7234.79 +/- 681.23\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.23e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.91      |\n",
      "| time/               |           |\n",
      "|    episodes         | 8         |\n",
      "|    fps              | 818       |\n",
      "|    time_elapsed     | 2         |\n",
      "|    total timesteps  | 1900      |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=2000, episode_reward=-6836.71 +/- 927.18\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=2500, episode_reward=-6768.97 +/- 696.24\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -6.77e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.862     |\n",
      "| time/               |           |\n",
      "|    episodes         | 12        |\n",
      "|    fps              | 889       |\n",
      "|    time_elapsed     | 3         |\n",
      "|    total timesteps  | 2900      |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=3000, episode_reward=-6554.81 +/- 751.05\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=3500, episode_reward=-7680.69 +/- 635.79\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.68e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.815     |\n",
      "| time/               |           |\n",
      "|    episodes         | 16        |\n",
      "|    fps              | 932       |\n",
      "|    time_elapsed     | 4         |\n",
      "|    total timesteps  | 3900      |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=4000, episode_reward=-7597.77 +/- 896.19\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=4500, episode_reward=-7328.34 +/- 837.42\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.33e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.767     |\n",
      "| time/               |           |\n",
      "|    episodes         | 20        |\n",
      "|    fps              | 978       |\n",
      "|    time_elapsed     | 5         |\n",
      "|    total timesteps  | 4900      |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=5000, episode_reward=-7395.17 +/- 519.36\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=5500, episode_reward=-7412.73 +/- 1574.43\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.41e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.72      |\n",
      "| time/               |           |\n",
      "|    episodes         | 24        |\n",
      "|    fps              | 1004      |\n",
      "|    time_elapsed     | 5         |\n",
      "|    total timesteps  | 5900      |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=6000, episode_reward=-6959.03 +/- 696.24\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=6500, episode_reward=-7230.98 +/- 462.95\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.23e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.672     |\n",
      "| time/               |           |\n",
      "|    episodes         | 28        |\n",
      "|    fps              | 1031      |\n",
      "|    time_elapsed     | 6         |\n",
      "|    total timesteps  | 6900      |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=7000, episode_reward=-7096.95 +/- 336.59\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=7500, episode_reward=-7351.91 +/- 755.37\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.35e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.625     |\n",
      "| time/               |           |\n",
      "|    episodes         | 32        |\n",
      "|    fps              | 1052      |\n",
      "|    time_elapsed     | 7         |\n",
      "|    total timesteps  | 7900      |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=8000, episode_reward=-7067.78 +/- 1021.81\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=8500, episode_reward=-7412.48 +/- 1098.28\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.41e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.577     |\n",
      "| time/               |           |\n",
      "|    episodes         | 36        |\n",
      "|    fps              | 1063      |\n",
      "|    time_elapsed     | 8         |\n",
      "|    total timesteps  | 8900      |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=9000, episode_reward=-7500.15 +/- 848.27\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=9500, episode_reward=-7052.93 +/- 731.16\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.05e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.53      |\n",
      "| time/               |           |\n",
      "|    episodes         | 40        |\n",
      "|    fps              | 1077      |\n",
      "|    time_elapsed     | 9         |\n",
      "|    total timesteps  | 9900      |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=10000, episode_reward=-6935.96 +/- 639.09\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=10500, episode_reward=-7037.41 +/- 711.77\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.04e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.482     |\n",
      "| time/               |           |\n",
      "|    episodes         | 44        |\n",
      "|    fps              | 1089      |\n",
      "|    time_elapsed     | 10        |\n",
      "|    total timesteps  | 10900     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=11000, episode_reward=-6761.86 +/- 692.31\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=11500, episode_reward=-6332.56 +/- 419.28\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -6.33e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.435     |\n",
      "| time/               |           |\n",
      "|    episodes         | 48        |\n",
      "|    fps              | 1098      |\n",
      "|    time_elapsed     | 10        |\n",
      "|    total timesteps  | 11900     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=12000, episode_reward=-7044.29 +/- 711.55\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=12500, episode_reward=-7458.83 +/- 831.73\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.46e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.387     |\n",
      "| time/               |           |\n",
      "|    episodes         | 52        |\n",
      "|    fps              | 1101      |\n",
      "|    time_elapsed     | 11        |\n",
      "|    total timesteps  | 12900     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=13000, episode_reward=-6925.99 +/- 717.94\n",
      "Episode length: 200.00 +/- 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=13500, episode_reward=-7236.10 +/- 828.88\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.24e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.34      |\n",
      "| time/               |           |\n",
      "|    episodes         | 56        |\n",
      "|    fps              | 1108      |\n",
      "|    time_elapsed     | 12        |\n",
      "|    total timesteps  | 13900     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=14000, episode_reward=-7185.32 +/- 1033.96\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=14500, episode_reward=-7068.00 +/- 799.04\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.07e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.292     |\n",
      "| time/               |           |\n",
      "|    episodes         | 60        |\n",
      "|    fps              | 1114      |\n",
      "|    time_elapsed     | 13        |\n",
      "|    total timesteps  | 14900     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=15000, episode_reward=-7001.23 +/- 624.43\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=15500, episode_reward=-7190.49 +/- 1026.12\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.19e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.245     |\n",
      "| time/               |           |\n",
      "|    episodes         | 64        |\n",
      "|    fps              | 1120      |\n",
      "|    time_elapsed     | 14        |\n",
      "|    total timesteps  | 15900     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=16000, episode_reward=-6764.86 +/- 764.93\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=16500, episode_reward=-6947.46 +/- 1153.29\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -6.95e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.197     |\n",
      "| time/               |           |\n",
      "|    episodes         | 68        |\n",
      "|    fps              | 1123      |\n",
      "|    time_elapsed     | 15        |\n",
      "|    total timesteps  | 16900     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=17000, episode_reward=-6605.65 +/- 620.60\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=17500, episode_reward=-7605.81 +/- 831.67\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.61e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.15      |\n",
      "| time/               |           |\n",
      "|    episodes         | 72        |\n",
      "|    fps              | 1122      |\n",
      "|    time_elapsed     | 15        |\n",
      "|    total timesteps  | 17900     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=18000, episode_reward=-6791.56 +/- 886.31\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=18500, episode_reward=-6337.65 +/- 518.67\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -6.34e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.102     |\n",
      "| time/               |           |\n",
      "|    episodes         | 76        |\n",
      "|    fps              | 1125      |\n",
      "|    time_elapsed     | 16        |\n",
      "|    total timesteps  | 18900     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=19000, episode_reward=-6817.56 +/- 1113.35\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=19500, episode_reward=-7458.66 +/- 526.81\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.46e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.0547    |\n",
      "| time/               |           |\n",
      "|    episodes         | 80        |\n",
      "|    fps              | 1125      |\n",
      "|    time_elapsed     | 17        |\n",
      "|    total timesteps  | 19900     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=20000, episode_reward=-6803.42 +/- 611.97\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=20500, episode_reward=-6386.84 +/- 849.83\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -6.39e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 84        |\n",
      "|    fps              | 1125      |\n",
      "|    time_elapsed     | 18        |\n",
      "|    total timesteps  | 20900     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=21000, episode_reward=-6831.97 +/- 627.48\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=21500, episode_reward=-6743.34 +/- 912.75\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -6.74e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 88        |\n",
      "|    fps              | 1123      |\n",
      "|    time_elapsed     | 19        |\n",
      "|    total timesteps  | 21900     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=22000, episode_reward=-7068.92 +/- 200.73\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=22500, episode_reward=-6477.27 +/- 856.65\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -6.48e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 92        |\n",
      "|    fps              | 1126      |\n",
      "|    time_elapsed     | 20        |\n",
      "|    total timesteps  | 22900     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=23000, episode_reward=-7591.47 +/- 777.12\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=23500, episode_reward=-7488.93 +/- 975.70\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.49e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 96        |\n",
      "|    fps              | 1129      |\n",
      "|    time_elapsed     | 21        |\n",
      "|    total timesteps  | 23900     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=24000, episode_reward=-6801.41 +/- 376.70\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=24500, episode_reward=-7014.88 +/- 881.73\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.01e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 100       |\n",
      "|    fps              | 1133      |\n",
      "|    time_elapsed     | 21        |\n",
      "|    total timesteps  | 24900     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=25000, episode_reward=-6161.99 +/- 763.80\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=25500, episode_reward=-6905.64 +/- 796.98\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -6.91e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 104       |\n",
      "|    fps              | 1133      |\n",
      "|    time_elapsed     | 22        |\n",
      "|    total timesteps  | 25900     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=26000, episode_reward=-6700.34 +/- 396.30\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=26500, episode_reward=-7042.89 +/- 629.24\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.04e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 108       |\n",
      "|    fps              | 1134      |\n",
      "|    time_elapsed     | 23        |\n",
      "|    total timesteps  | 26900     |\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=27000, episode_reward=-7606.62 +/- 1084.72\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=27500, episode_reward=-7562.77 +/- 627.28\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.56e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 112       |\n",
      "|    fps              | 1132      |\n",
      "|    time_elapsed     | 24        |\n",
      "|    total timesteps  | 27900     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=28000, episode_reward=-7257.33 +/- 666.69\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=28500, episode_reward=-6867.85 +/- 851.94\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -6.87e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 116       |\n",
      "|    fps              | 1135      |\n",
      "|    time_elapsed     | 25        |\n",
      "|    total timesteps  | 28900     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=29000, episode_reward=-7284.33 +/- 880.70\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=29500, episode_reward=-6803.65 +/- 920.66\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -6.8e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 120      |\n",
      "|    fps              | 1136     |\n",
      "|    time_elapsed     | 26       |\n",
      "|    total timesteps  | 29900    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=-6658.68 +/- 1133.12\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=30500, episode_reward=-7203.27 +/- 572.04\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -7.2e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 124      |\n",
      "|    fps              | 1137     |\n",
      "|    time_elapsed     | 27       |\n",
      "|    total timesteps  | 30900    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=31000, episode_reward=-7660.81 +/- 685.65\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=31500, episode_reward=-7285.44 +/- 592.07\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.29e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 128       |\n",
      "|    fps              | 1140      |\n",
      "|    time_elapsed     | 27        |\n",
      "|    total timesteps  | 31900     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=32000, episode_reward=-7058.06 +/- 353.27\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=32500, episode_reward=-6745.71 +/- 658.07\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -6.75e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 132       |\n",
      "|    fps              | 1143      |\n",
      "|    time_elapsed     | 28        |\n",
      "|    total timesteps  | 32900     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=33000, episode_reward=-7893.38 +/- 1017.02\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=33500, episode_reward=-6967.06 +/- 908.02\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -6.97e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 136       |\n",
      "|    fps              | 1144      |\n",
      "|    time_elapsed     | 29        |\n",
      "|    total timesteps  | 33900     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=34000, episode_reward=-6943.80 +/- 449.11\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=34500, episode_reward=-6824.78 +/- 787.21\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -6.82e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 140       |\n",
      "|    fps              | 1144      |\n",
      "|    time_elapsed     | 30        |\n",
      "|    total timesteps  | 34900     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=35000, episode_reward=-6958.50 +/- 771.68\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=35500, episode_reward=-7074.70 +/- 475.68\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.07e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 144       |\n",
      "|    fps              | 1141      |\n",
      "|    time_elapsed     | 31        |\n",
      "|    total timesteps  | 35900     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=36000, episode_reward=-6893.48 +/- 882.99\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=36500, episode_reward=-6858.25 +/- 583.13\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -6.86e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 148       |\n",
      "|    fps              | 1140      |\n",
      "|    time_elapsed     | 32        |\n",
      "|    total timesteps  | 36900     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=37000, episode_reward=-7615.78 +/- 363.06\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=37500, episode_reward=-7060.70 +/- 811.49\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.06e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 152       |\n",
      "|    fps              | 1139      |\n",
      "|    time_elapsed     | 33        |\n",
      "|    total timesteps  | 37900     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=38000, episode_reward=-6486.77 +/- 803.74\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=38500, episode_reward=-6998.26 +/- 355.24\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -7e+03   |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 156      |\n",
      "|    fps              | 1141     |\n",
      "|    time_elapsed     | 34       |\n",
      "|    total timesteps  | 38900    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=39000, episode_reward=-7712.90 +/- 548.74\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=39500, episode_reward=-7089.28 +/- 928.25\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.09e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 160       |\n",
      "|    fps              | 1141      |\n",
      "|    time_elapsed     | 34        |\n",
      "|    total timesteps  | 39900     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=-7494.01 +/- 808.39\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=40500, episode_reward=-7194.17 +/- 973.69\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.19e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 164       |\n",
      "|    fps              | 1142      |\n",
      "|    time_elapsed     | 35        |\n",
      "|    total timesteps  | 40900     |\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=41000, episode_reward=-6777.13 +/- 406.30\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=41500, episode_reward=-7126.24 +/- 680.43\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.13e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 168       |\n",
      "|    fps              | 1144      |\n",
      "|    time_elapsed     | 36        |\n",
      "|    total timesteps  | 41900     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=42000, episode_reward=-6903.62 +/- 469.13\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=42500, episode_reward=-6807.04 +/- 883.80\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -6.81e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 172       |\n",
      "|    fps              | 1142      |\n",
      "|    time_elapsed     | 37        |\n",
      "|    total timesteps  | 42900     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=43000, episode_reward=-6958.43 +/- 711.85\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=43500, episode_reward=-7189.03 +/- 233.88\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.19e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 176       |\n",
      "|    fps              | 1143      |\n",
      "|    time_elapsed     | 38        |\n",
      "|    total timesteps  | 43900     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=44000, episode_reward=-7023.21 +/- 285.28\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=44500, episode_reward=-6475.08 +/- 308.14\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -6.48e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 180       |\n",
      "|    fps              | 1145      |\n",
      "|    time_elapsed     | 39        |\n",
      "|    total timesteps  | 44900     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=45000, episode_reward=-6939.91 +/- 345.22\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=45500, episode_reward=-7280.79 +/- 928.20\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.28e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 184       |\n",
      "|    fps              | 1146      |\n",
      "|    time_elapsed     | 40        |\n",
      "|    total timesteps  | 45900     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=46000, episode_reward=-6956.21 +/- 536.31\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=46500, episode_reward=-6905.92 +/- 1013.35\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -6.91e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 188       |\n",
      "|    fps              | 1147      |\n",
      "|    time_elapsed     | 40        |\n",
      "|    total timesteps  | 46900     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=47000, episode_reward=-6904.09 +/- 761.18\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=47500, episode_reward=-6604.54 +/- 738.64\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -6.6e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 192      |\n",
      "|    fps              | 1146     |\n",
      "|    time_elapsed     | 41       |\n",
      "|    total timesteps  | 47900    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=48000, episode_reward=-7157.86 +/- 587.96\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=48500, episode_reward=-7686.65 +/- 772.94\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -7.69e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 196       |\n",
      "|    fps              | 1147      |\n",
      "|    time_elapsed     | 42        |\n",
      "|    total timesteps  | 48900     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=49000, episode_reward=-7053.71 +/- 1233.32\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=49500, episode_reward=-6560.20 +/- 788.85\n",
      "Episode length: 200.00 +/- 0.00\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -6.56e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 200       |\n",
      "|    fps              | 1146      |\n",
      "|    time_elapsed     | 43        |\n",
      "|    total timesteps  | 49900     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=-6363.24 +/- 511.40\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=50500, episode_reward=-5581.29 +/- 967.03\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-----------------------------------\n",
      "| eval/               |           |\n",
      "|    mean_ep_length   | 200       |\n",
      "|    mean_reward      | -5.58e+03 |\n",
      "| rollout/            |           |\n",
      "|    exploration rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 204       |\n",
      "|    fps              | 1124      |\n",
      "|    time_elapsed     | 45        |\n",
      "|    total timesteps  | 50900     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 6.78      |\n",
      "|    n_updates        | 224       |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=51000, episode_reward=-4417.27 +/- 555.70\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=51500, episode_reward=-302.92 +/- 28.44\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -303     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 208      |\n",
      "|    fps              | 1103     |\n",
      "|    time_elapsed     | 47       |\n",
      "|    total timesteps  | 51900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 5.43     |\n",
      "|    n_updates        | 474      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=52000, episode_reward=-2755.42 +/- 348.41\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=52500, episode_reward=-246.38 +/- 72.69\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -246     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 212      |\n",
      "|    fps              | 1085     |\n",
      "|    time_elapsed     | 48       |\n",
      "|    total timesteps  | 52900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.81     |\n",
      "|    n_updates        | 724      |\n",
      "----------------------------------\n",
      "Eval num_timesteps=53000, episode_reward=-537.20 +/- 55.97\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=53500, episode_reward=-292.59 +/- 48.39\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -293     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 216      |\n",
      "|    fps              | 1066     |\n",
      "|    time_elapsed     | 50       |\n",
      "|    total timesteps  | 53900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.29     |\n",
      "|    n_updates        | 974      |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=54000, episode_reward=6.26 +/- 14.74\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=54500, episode_reward=-156.95 +/- 142.79\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -157     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 220      |\n",
      "|    fps              | 1051     |\n",
      "|    time_elapsed     | 52       |\n",
      "|    total timesteps  | 54900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.59     |\n",
      "|    n_updates        | 1224     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=55000, episode_reward=-586.12 +/- 50.57\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=55500, episode_reward=-227.44 +/- 116.96\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -227     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 224      |\n",
      "|    fps              | 1034     |\n",
      "|    time_elapsed     | 54       |\n",
      "|    total timesteps  | 55900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.3      |\n",
      "|    n_updates        | 1474     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=56000, episode_reward=15.07 +/- 6.05\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=56500, episode_reward=12.53 +/- 6.75\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 12.5     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 228      |\n",
      "|    fps              | 1019     |\n",
      "|    time_elapsed     | 55       |\n",
      "|    total timesteps  | 56900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.21     |\n",
      "|    n_updates        | 1724     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=57000, episode_reward=27.30 +/- 5.97\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "Eval num_timesteps=57500, episode_reward=-17.61 +/- 9.99\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -17.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 232      |\n",
      "|    fps              | 1006     |\n",
      "|    time_elapsed     | 57       |\n",
      "|    total timesteps  | 57900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.795    |\n",
      "|    n_updates        | 1974     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=58000, episode_reward=-587.74 +/- 44.07\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=58500, episode_reward=-539.49 +/- 54.03\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -539     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 236      |\n",
      "|    fps              | 992      |\n",
      "|    time_elapsed     | 59       |\n",
      "|    total timesteps  | 58900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.973    |\n",
      "|    n_updates        | 2224     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=59000, episode_reward=20.61 +/- 5.43\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=59500, episode_reward=14.28 +/- 7.26\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 14.3     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 240      |\n",
      "|    fps              | 980      |\n",
      "|    time_elapsed     | 61       |\n",
      "|    total timesteps  | 59900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.559    |\n",
      "|    n_updates        | 2474     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=21.03 +/- 3.25\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=60500, episode_reward=17.67 +/- 5.27\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 17.7     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 244      |\n",
      "|    fps              | 969      |\n",
      "|    time_elapsed     | 62       |\n",
      "|    total timesteps  | 60900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.32     |\n",
      "|    n_updates        | 2724     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=61000, episode_reward=14.17 +/- 10.04\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=61500, episode_reward=13.37 +/- 3.33\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 13.4     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 248      |\n",
      "|    fps              | 956      |\n",
      "|    time_elapsed     | 64       |\n",
      "|    total timesteps  | 61900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.54     |\n",
      "|    n_updates        | 2974     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=62000, episode_reward=25.12 +/- 4.07\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=62500, episode_reward=4.16 +/- 8.47\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 4.16     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 252      |\n",
      "|    fps              | 945      |\n",
      "|    time_elapsed     | 66       |\n",
      "|    total timesteps  | 62900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.44     |\n",
      "|    n_updates        | 3224     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=63000, episode_reward=21.68 +/- 6.77\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=63500, episode_reward=16.36 +/- 7.72\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 16.4     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 256      |\n",
      "|    fps              | 936      |\n",
      "|    time_elapsed     | 68       |\n",
      "|    total timesteps  | 63900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.883    |\n",
      "|    n_updates        | 3474     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=64000, episode_reward=-58.22 +/- 95.94\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=64500, episode_reward=-211.61 +/- 260.01\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -212     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 260      |\n",
      "|    fps              | 925      |\n",
      "|    time_elapsed     | 70       |\n",
      "|    total timesteps  | 64900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.875    |\n",
      "|    n_updates        | 3724     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=65000, episode_reward=23.15 +/- 6.84\n",
      "Episode length: 200.00 +/- 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=65500, episode_reward=-42.93 +/- 87.58\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -42.9    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 264      |\n",
      "|    fps              | 916      |\n",
      "|    time_elapsed     | 71       |\n",
      "|    total timesteps  | 65900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.857    |\n",
      "|    n_updates        | 3974     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=66000, episode_reward=2.03 +/- 10.28\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=66500, episode_reward=20.53 +/- 8.82\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 20.5     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 268      |\n",
      "|    fps              | 907      |\n",
      "|    time_elapsed     | 73       |\n",
      "|    total timesteps  | 66900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.438    |\n",
      "|    n_updates        | 4224     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=67000, episode_reward=12.87 +/- 17.28\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=67500, episode_reward=24.47 +/- 2.04\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 24.5     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 272      |\n",
      "|    fps              | 899      |\n",
      "|    time_elapsed     | 75       |\n",
      "|    total timesteps  | 67900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.498    |\n",
      "|    n_updates        | 4474     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=68000, episode_reward=25.39 +/- 0.99\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=68500, episode_reward=22.49 +/- 7.06\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 22.5     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 276      |\n",
      "|    fps              | 893      |\n",
      "|    time_elapsed     | 77       |\n",
      "|    total timesteps  | 68900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.544    |\n",
      "|    n_updates        | 4724     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=69000, episode_reward=-33.53 +/- 13.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=69500, episode_reward=-22.93 +/- 5.13\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -22.9    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 280      |\n",
      "|    fps              | 886      |\n",
      "|    time_elapsed     | 78       |\n",
      "|    total timesteps  | 69900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.57     |\n",
      "|    n_updates        | 4974     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=21.52 +/- 8.85\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=70500, episode_reward=16.69 +/- 10.58\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 16.7     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 284      |\n",
      "|    fps              | 880      |\n",
      "|    time_elapsed     | 80       |\n",
      "|    total timesteps  | 70900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 4.37     |\n",
      "|    n_updates        | 5224     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=71000, episode_reward=25.93 +/- 7.06\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=71500, episode_reward=25.18 +/- 5.07\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 25.2     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 288      |\n",
      "|    fps              | 874      |\n",
      "|    time_elapsed     | 82       |\n",
      "|    total timesteps  | 71900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.839    |\n",
      "|    n_updates        | 5474     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=72000, episode_reward=-375.94 +/- 290.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=72500, episode_reward=-46.21 +/- 75.85\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -46.2    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 292      |\n",
      "|    fps              | 869      |\n",
      "|    time_elapsed     | 83       |\n",
      "|    total timesteps  | 72900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.481    |\n",
      "|    n_updates        | 5724     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=73000, episode_reward=-4.31 +/- 29.61\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=73500, episode_reward=22.50 +/- 8.17\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 22.5     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 296      |\n",
      "|    fps              | 863      |\n",
      "|    time_elapsed     | 85       |\n",
      "|    total timesteps  | 73900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.479    |\n",
      "|    n_updates        | 5974     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=74000, episode_reward=23.32 +/- 4.26\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=74500, episode_reward=7.73 +/- 15.93\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 7.73     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 300      |\n",
      "|    fps              | 858      |\n",
      "|    time_elapsed     | 87       |\n",
      "|    total timesteps  | 74900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.538    |\n",
      "|    n_updates        | 6224     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=75000, episode_reward=14.14 +/- 4.76\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=75500, episode_reward=27.47 +/- 3.01\n",
      "Episode length: 200.00 +/- 0.00\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 27.5     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 304      |\n",
      "|    fps              | 853      |\n",
      "|    time_elapsed     | 88       |\n",
      "|    total timesteps  | 75900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.547    |\n",
      "|    n_updates        | 6474     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=76000, episode_reward=0.20 +/- 16.04\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=76500, episode_reward=14.85 +/- 6.86\n",
      "Episode length: 200.00 +/- 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 14.8     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 308      |\n",
      "|    fps              | 848      |\n",
      "|    time_elapsed     | 90       |\n",
      "|    total timesteps  | 76900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.25     |\n",
      "|    n_updates        | 6724     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=77000, episode_reward=0.85 +/- 37.07\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=77500, episode_reward=17.89 +/- 9.10\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 17.9     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 312      |\n",
      "|    fps              | 843      |\n",
      "|    time_elapsed     | 92       |\n",
      "|    total timesteps  | 77900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.321    |\n",
      "|    n_updates        | 6974     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=78000, episode_reward=21.03 +/- 5.44\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=78500, episode_reward=-16.45 +/- 35.50\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -16.5    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 316      |\n",
      "|    fps              | 836      |\n",
      "|    time_elapsed     | 94       |\n",
      "|    total timesteps  | 78900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.421    |\n",
      "|    n_updates        | 7224     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=79000, episode_reward=-1.31 +/- 41.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=79500, episode_reward=-70.98 +/- 8.97\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -71      |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 320      |\n",
      "|    fps              | 830      |\n",
      "|    time_elapsed     | 96       |\n",
      "|    total timesteps  | 79900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.566    |\n",
      "|    n_updates        | 7474     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=-0.08 +/- 16.72\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=80500, episode_reward=-22.55 +/- 35.38\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -22.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 324      |\n",
      "|    fps              | 826      |\n",
      "|    time_elapsed     | 97       |\n",
      "|    total timesteps  | 80900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.773    |\n",
      "|    n_updates        | 7724     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=81000, episode_reward=9.35 +/- 24.35\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=81500, episode_reward=-27.61 +/- 12.99\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -27.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 328      |\n",
      "|    fps              | 821      |\n",
      "|    time_elapsed     | 99       |\n",
      "|    total timesteps  | 81900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.871    |\n",
      "|    n_updates        | 7974     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=82000, episode_reward=21.23 +/- 3.10\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=82500, episode_reward=17.12 +/- 13.20\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 17.1     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 332      |\n",
      "|    fps              | 816      |\n",
      "|    time_elapsed     | 101      |\n",
      "|    total timesteps  | 82900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.935    |\n",
      "|    n_updates        | 8224     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=83000, episode_reward=-90.96 +/- 71.44\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=83500, episode_reward=-120.00 +/- 23.78\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -120     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 336      |\n",
      "|    fps              | 812      |\n",
      "|    time_elapsed     | 103      |\n",
      "|    total timesteps  | 83900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.585    |\n",
      "|    n_updates        | 8474     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=84000, episode_reward=-4.49 +/- 14.46\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=84500, episode_reward=-18.16 +/- 88.70\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -18.2    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 340      |\n",
      "|    fps              | 807      |\n",
      "|    time_elapsed     | 105      |\n",
      "|    total timesteps  | 84900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.592    |\n",
      "|    n_updates        | 8724     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=85000, episode_reward=16.01 +/- 14.26\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=85500, episode_reward=6.32 +/- 12.32\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 6.32     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 344      |\n",
      "|    fps              | 803      |\n",
      "|    time_elapsed     | 106      |\n",
      "|    total timesteps  | 85900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.595    |\n",
      "|    n_updates        | 8974     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=86000, episode_reward=-162.04 +/- 314.34\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=86500, episode_reward=-45.83 +/- 50.61\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -45.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 348      |\n",
      "|    fps              | 799      |\n",
      "|    time_elapsed     | 108      |\n",
      "|    total timesteps  | 86900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.53     |\n",
      "|    n_updates        | 9224     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=87000, episode_reward=-21.93 +/- 52.22\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=87500, episode_reward=4.58 +/- 4.80\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 4.58     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 352      |\n",
      "|    fps              | 795      |\n",
      "|    time_elapsed     | 110      |\n",
      "|    total timesteps  | 87900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.441    |\n",
      "|    n_updates        | 9474     |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=88000, episode_reward=-129.98 +/- 47.22\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=88500, episode_reward=-220.60 +/- 163.55\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -221     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 356      |\n",
      "|    fps              | 791      |\n",
      "|    time_elapsed     | 112      |\n",
      "|    total timesteps  | 88900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.321    |\n",
      "|    n_updates        | 9724     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=89000, episode_reward=21.44 +/- 4.06\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=89500, episode_reward=7.05 +/- 20.25\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 7.05     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 360      |\n",
      "|    fps              | 787      |\n",
      "|    time_elapsed     | 114      |\n",
      "|    total timesteps  | 89900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.606    |\n",
      "|    n_updates        | 9974     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=90000, episode_reward=-45.72 +/- 69.77\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=90500, episode_reward=10.41 +/- 20.29\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 10.4     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 364      |\n",
      "|    fps              | 784      |\n",
      "|    time_elapsed     | 115      |\n",
      "|    total timesteps  | 90900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.579    |\n",
      "|    n_updates        | 10224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=91000, episode_reward=-60.63 +/- 122.15\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=91500, episode_reward=-42.13 +/- 98.38\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -42.1    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 368      |\n",
      "|    fps              | 780      |\n",
      "|    time_elapsed     | 117      |\n",
      "|    total timesteps  | 91900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.993    |\n",
      "|    n_updates        | 10474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=92000, episode_reward=3.45 +/- 29.07\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=92500, episode_reward=12.92 +/- 16.92\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 12.9     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 372      |\n",
      "|    fps              | 778      |\n",
      "|    time_elapsed     | 119      |\n",
      "|    total timesteps  | 92900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.378    |\n",
      "|    n_updates        | 10724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=93000, episode_reward=-70.06 +/- 84.26\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=93500, episode_reward=18.31 +/- 4.81\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 18.3     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 376      |\n",
      "|    fps              | 774      |\n",
      "|    time_elapsed     | 121      |\n",
      "|    total timesteps  | 93900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.33     |\n",
      "|    n_updates        | 10974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=94000, episode_reward=7.10 +/- 29.65\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=94500, episode_reward=-138.63 +/- 117.06\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -139     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 380      |\n",
      "|    fps              | 772      |\n",
      "|    time_elapsed     | 122      |\n",
      "|    total timesteps  | 94900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.501    |\n",
      "|    n_updates        | 11224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=95000, episode_reward=8.19 +/- 4.20\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=95500, episode_reward=-41.75 +/- 50.30\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -41.8    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 384      |\n",
      "|    fps              | 768      |\n",
      "|    time_elapsed     | 124      |\n",
      "|    total timesteps  | 95900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.773    |\n",
      "|    n_updates        | 11474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=96000, episode_reward=-963.74 +/- 534.63\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=96500, episode_reward=-195.61 +/- 79.97\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -196     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 388      |\n",
      "|    fps              | 764      |\n",
      "|    time_elapsed     | 126      |\n",
      "|    total timesteps  | 96900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.29     |\n",
      "|    n_updates        | 11724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=97000, episode_reward=-981.88 +/- 51.63\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=97500, episode_reward=-14.31 +/- 18.04\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -14.3    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 392      |\n",
      "|    fps              | 761      |\n",
      "|    time_elapsed     | 128      |\n",
      "|    total timesteps  | 97900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.386    |\n",
      "|    n_updates        | 11974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=98000, episode_reward=-156.14 +/- 346.21\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=98500, episode_reward=-7.87 +/- 14.42\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -7.87    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 396      |\n",
      "|    fps              | 758      |\n",
      "|    time_elapsed     | 130      |\n",
      "|    total timesteps  | 98900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.658    |\n",
      "|    n_updates        | 12224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=99000, episode_reward=-6.79 +/- 28.91\n",
      "Episode length: 200.00 +/- 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=99500, episode_reward=-13.94 +/- 7.07\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -13.9    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 400      |\n",
      "|    fps              | 754      |\n",
      "|    time_elapsed     | 132      |\n",
      "|    total timesteps  | 99900    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.427    |\n",
      "|    n_updates        | 12474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=8.37 +/- 6.67\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=100500, episode_reward=4.50 +/- 24.79\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 4.5      |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 404      |\n",
      "|    fps              | 752      |\n",
      "|    time_elapsed     | 134      |\n",
      "|    total timesteps  | 100900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.869    |\n",
      "|    n_updates        | 12724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=101000, episode_reward=-430.40 +/- 348.51\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=101500, episode_reward=2.92 +/- 10.78\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 2.92     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 408      |\n",
      "|    fps              | 750      |\n",
      "|    time_elapsed     | 135      |\n",
      "|    total timesteps  | 101900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.414    |\n",
      "|    n_updates        | 12974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=102000, episode_reward=5.16 +/- 10.18\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=102500, episode_reward=9.34 +/- 11.14\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 9.34     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 412      |\n",
      "|    fps              | 747      |\n",
      "|    time_elapsed     | 137      |\n",
      "|    total timesteps  | 102900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.356    |\n",
      "|    n_updates        | 13224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=103000, episode_reward=-320.96 +/- 324.93\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=103500, episode_reward=-10.95 +/- 30.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -10.9    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 416      |\n",
      "|    fps              | 745      |\n",
      "|    time_elapsed     | 139      |\n",
      "|    total timesteps  | 103900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.99     |\n",
      "|    n_updates        | 13474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=104000, episode_reward=9.44 +/- 15.84\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=104500, episode_reward=-1.65 +/- 20.62\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -1.65    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 420      |\n",
      "|    fps              | 743      |\n",
      "|    time_elapsed     | 141      |\n",
      "|    total timesteps  | 104900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.6      |\n",
      "|    n_updates        | 13724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=105000, episode_reward=-465.00 +/- 403.41\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=105500, episode_reward=-186.87 +/- 200.38\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -187     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 424      |\n",
      "|    fps              | 741      |\n",
      "|    time_elapsed     | 142      |\n",
      "|    total timesteps  | 105900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.469    |\n",
      "|    n_updates        | 13974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=106000, episode_reward=-198.22 +/- 46.67\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=106500, episode_reward=13.75 +/- 8.14\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 13.8     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 428      |\n",
      "|    fps              | 740      |\n",
      "|    time_elapsed     | 144      |\n",
      "|    total timesteps  | 106900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.33     |\n",
      "|    n_updates        | 14224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=107000, episode_reward=16.85 +/- 7.82\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=107500, episode_reward=-552.44 +/- 337.49\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -552     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 432      |\n",
      "|    fps              | 738      |\n",
      "|    time_elapsed     | 146      |\n",
      "|    total timesteps  | 107900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.174    |\n",
      "|    n_updates        | 14474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=108000, episode_reward=-286.04 +/- 229.61\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=108500, episode_reward=-36.05 +/- 68.70\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -36.1    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 436      |\n",
      "|    fps              | 737      |\n",
      "|    time_elapsed     | 147      |\n",
      "|    total timesteps  | 108900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.433    |\n",
      "|    n_updates        | 14724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=109000, episode_reward=-642.92 +/- 105.15\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=109500, episode_reward=-127.24 +/- 83.16\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -127     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 440      |\n",
      "|    fps              | 735      |\n",
      "|    time_elapsed     | 149      |\n",
      "|    total timesteps  | 109900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.15     |\n",
      "|    n_updates        | 14974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=110000, episode_reward=-178.97 +/- 163.80\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=110500, episode_reward=6.29 +/- 10.58\n",
      "Episode length: 200.00 +/- 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 6.29     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 444      |\n",
      "|    fps              | 734      |\n",
      "|    time_elapsed     | 150      |\n",
      "|    total timesteps  | 110900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.842    |\n",
      "|    n_updates        | 15224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=111000, episode_reward=-8.41 +/- 47.40\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=111500, episode_reward=-10.54 +/- 66.27\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -10.5    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 448      |\n",
      "|    fps              | 732      |\n",
      "|    time_elapsed     | 152      |\n",
      "|    total timesteps  | 111900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.864    |\n",
      "|    n_updates        | 15474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=112000, episode_reward=-41.81 +/- 101.58\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=112500, episode_reward=0.62 +/- 41.46\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 0.622    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 452      |\n",
      "|    fps              | 731      |\n",
      "|    time_elapsed     | 154      |\n",
      "|    total timesteps  | 112900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.528    |\n",
      "|    n_updates        | 15724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=113000, episode_reward=18.19 +/- 5.33\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=113500, episode_reward=-5.24 +/- 21.43\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -5.24    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 456      |\n",
      "|    fps              | 730      |\n",
      "|    time_elapsed     | 155      |\n",
      "|    total timesteps  | 113900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.2      |\n",
      "|    n_updates        | 15974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=114000, episode_reward=-744.80 +/- 359.40\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=114500, episode_reward=-23.42 +/- 38.91\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -23.4    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 460      |\n",
      "|    fps              | 729      |\n",
      "|    time_elapsed     | 157      |\n",
      "|    total timesteps  | 114900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.597    |\n",
      "|    n_updates        | 16224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=115000, episode_reward=-64.09 +/- 75.98\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=115500, episode_reward=7.24 +/- 13.57\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 7.24     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 464      |\n",
      "|    fps              | 727      |\n",
      "|    time_elapsed     | 159      |\n",
      "|    total timesteps  | 115900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.429    |\n",
      "|    n_updates        | 16474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=116000, episode_reward=-14.27 +/- 16.49\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=116500, episode_reward=-18.93 +/- 4.89\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -18.9    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 468      |\n",
      "|    fps              | 726      |\n",
      "|    time_elapsed     | 160      |\n",
      "|    total timesteps  | 116900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.228    |\n",
      "|    n_updates        | 16724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=117000, episode_reward=14.02 +/- 8.72\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=117500, episode_reward=-42.71 +/- 87.43\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -42.7    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 472      |\n",
      "|    fps              | 725      |\n",
      "|    time_elapsed     | 162      |\n",
      "|    total timesteps  | 117900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.797    |\n",
      "|    n_updates        | 16974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=118000, episode_reward=-4.77 +/- 9.75\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=118500, episode_reward=9.36 +/- 12.96\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 9.36     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 476      |\n",
      "|    fps              | 723      |\n",
      "|    time_elapsed     | 164      |\n",
      "|    total timesteps  | 118900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.309    |\n",
      "|    n_updates        | 17224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=119000, episode_reward=-52.52 +/- 79.31\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=119500, episode_reward=-22.03 +/- 17.65\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -22      |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 480      |\n",
      "|    fps              | 722      |\n",
      "|    time_elapsed     | 165      |\n",
      "|    total timesteps  | 119900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.706    |\n",
      "|    n_updates        | 17474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=120000, episode_reward=10.04 +/- 16.22\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=120500, episode_reward=16.09 +/- 5.32\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 16.1     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 484      |\n",
      "|    fps              | 721      |\n",
      "|    time_elapsed     | 167      |\n",
      "|    total timesteps  | 120900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.223    |\n",
      "|    n_updates        | 17724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=121000, episode_reward=12.48 +/- 9.17\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=121500, episode_reward=6.37 +/- 5.63\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 6.37     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 488      |\n",
      "|    fps              | 720      |\n",
      "|    time_elapsed     | 169      |\n",
      "|    total timesteps  | 121900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.279    |\n",
      "|    n_updates        | 17974    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=122000, episode_reward=-1.35 +/- 7.15\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=122500, episode_reward=-240.62 +/- 24.24\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -241     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 492      |\n",
      "|    fps              | 719      |\n",
      "|    time_elapsed     | 170      |\n",
      "|    total timesteps  | 122900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.433    |\n",
      "|    n_updates        | 18224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=123000, episode_reward=11.52 +/- 9.95\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=123500, episode_reward=15.80 +/- 11.07\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 15.8     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 496      |\n",
      "|    fps              | 718      |\n",
      "|    time_elapsed     | 172      |\n",
      "|    total timesteps  | 123900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.21     |\n",
      "|    n_updates        | 18474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=124000, episode_reward=-55.44 +/- 108.55\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=124500, episode_reward=9.10 +/- 9.61\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 9.1      |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 500      |\n",
      "|    fps              | 717      |\n",
      "|    time_elapsed     | 174      |\n",
      "|    total timesteps  | 124900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.363    |\n",
      "|    n_updates        | 18724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=125000, episode_reward=5.52 +/- 23.46\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=125500, episode_reward=21.42 +/- 6.95\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 21.4     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 504      |\n",
      "|    fps              | 716      |\n",
      "|    time_elapsed     | 175      |\n",
      "|    total timesteps  | 125900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.51     |\n",
      "|    n_updates        | 18974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=126000, episode_reward=-98.59 +/- 94.21\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=126500, episode_reward=13.77 +/- 15.43\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 13.8     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 508      |\n",
      "|    fps              | 714      |\n",
      "|    time_elapsed     | 177      |\n",
      "|    total timesteps  | 126900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.727    |\n",
      "|    n_updates        | 19224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=127000, episode_reward=16.33 +/- 8.04\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=127500, episode_reward=-297.46 +/- 18.97\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -297     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 512      |\n",
      "|    fps              | 714      |\n",
      "|    time_elapsed     | 179      |\n",
      "|    total timesteps  | 127900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.588    |\n",
      "|    n_updates        | 19474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=128000, episode_reward=-23.31 +/- 36.12\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=128500, episode_reward=-107.37 +/- 88.68\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -107     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 516      |\n",
      "|    fps              | 713      |\n",
      "|    time_elapsed     | 180      |\n",
      "|    total timesteps  | 128900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.256    |\n",
      "|    n_updates        | 19724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=129000, episode_reward=-15.64 +/- 54.02\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=129500, episode_reward=-99.55 +/- 150.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -99.5    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 520      |\n",
      "|    fps              | 712      |\n",
      "|    time_elapsed     | 182      |\n",
      "|    total timesteps  | 129900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.02     |\n",
      "|    n_updates        | 19974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=130000, episode_reward=-163.29 +/- 162.96\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=130500, episode_reward=9.16 +/- 14.75\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 9.16     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 524      |\n",
      "|    fps              | 711      |\n",
      "|    time_elapsed     | 184      |\n",
      "|    total timesteps  | 130900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.469    |\n",
      "|    n_updates        | 20224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=131000, episode_reward=-27.81 +/- 89.22\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=131500, episode_reward=-197.19 +/- 346.31\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -197     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 528      |\n",
      "|    fps              | 710      |\n",
      "|    time_elapsed     | 185      |\n",
      "|    total timesteps  | 131900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.01     |\n",
      "|    n_updates        | 20474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=132000, episode_reward=-159.58 +/- 312.60\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=132500, episode_reward=20.31 +/- 5.46\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 20.3     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 532      |\n",
      "|    fps              | 709      |\n",
      "|    time_elapsed     | 187      |\n",
      "|    total timesteps  | 132900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.725    |\n",
      "|    n_updates        | 20724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=133000, episode_reward=-13.14 +/- 12.08\n",
      "Episode length: 200.00 +/- 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=133500, episode_reward=-0.24 +/- 11.61\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -0.24    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 536      |\n",
      "|    fps              | 708      |\n",
      "|    time_elapsed     | 189      |\n",
      "|    total timesteps  | 133900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.405    |\n",
      "|    n_updates        | 20974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=134000, episode_reward=6.09 +/- 23.69\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=134500, episode_reward=-3.32 +/- 11.56\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -3.32    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 540      |\n",
      "|    fps              | 707      |\n",
      "|    time_elapsed     | 190      |\n",
      "|    total timesteps  | 134900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.392    |\n",
      "|    n_updates        | 21224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=135000, episode_reward=15.01 +/- 2.53\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=135500, episode_reward=10.45 +/- 5.08\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 10.5     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 544      |\n",
      "|    fps              | 706      |\n",
      "|    time_elapsed     | 192      |\n",
      "|    total timesteps  | 135900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.21     |\n",
      "|    n_updates        | 21474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=136000, episode_reward=-25.73 +/- 11.50\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=136500, episode_reward=11.12 +/- 7.85\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 11.1     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 548      |\n",
      "|    fps              | 704      |\n",
      "|    time_elapsed     | 194      |\n",
      "|    total timesteps  | 136900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.302    |\n",
      "|    n_updates        | 21724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=137000, episode_reward=15.05 +/- 10.39\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=137500, episode_reward=11.69 +/- 7.49\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 11.7     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 552      |\n",
      "|    fps              | 703      |\n",
      "|    time_elapsed     | 195      |\n",
      "|    total timesteps  | 137900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.746    |\n",
      "|    n_updates        | 21974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=138000, episode_reward=11.29 +/- 11.86\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=138500, episode_reward=-295.66 +/- 79.69\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -296     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 556      |\n",
      "|    fps              | 702      |\n",
      "|    time_elapsed     | 197      |\n",
      "|    total timesteps  | 138900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.751    |\n",
      "|    n_updates        | 22224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=139000, episode_reward=-13.13 +/- 10.47\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=139500, episode_reward=15.84 +/- 5.33\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 15.8     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 560      |\n",
      "|    fps              | 701      |\n",
      "|    time_elapsed     | 199      |\n",
      "|    total timesteps  | 139900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.843    |\n",
      "|    n_updates        | 22474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=140000, episode_reward=15.38 +/- 11.61\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=140500, episode_reward=-1.47 +/- 17.56\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -1.47    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 564      |\n",
      "|    fps              | 700      |\n",
      "|    time_elapsed     | 201      |\n",
      "|    total timesteps  | 140900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.736    |\n",
      "|    n_updates        | 22724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=141000, episode_reward=4.60 +/- 25.89\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=141500, episode_reward=17.63 +/- 10.46\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 17.6     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 568      |\n",
      "|    fps              | 699      |\n",
      "|    time_elapsed     | 202      |\n",
      "|    total timesteps  | 141900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.42     |\n",
      "|    n_updates        | 22974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=142000, episode_reward=-19.13 +/- 9.03\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=142500, episode_reward=19.84 +/- 8.17\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 19.8     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 572      |\n",
      "|    fps              | 698      |\n",
      "|    time_elapsed     | 204      |\n",
      "|    total timesteps  | 142900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.809    |\n",
      "|    n_updates        | 23224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=143000, episode_reward=-14.16 +/- 34.77\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=143500, episode_reward=15.54 +/- 10.33\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 15.5     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 576      |\n",
      "|    fps              | 697      |\n",
      "|    time_elapsed     | 206      |\n",
      "|    total timesteps  | 143900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.522    |\n",
      "|    n_updates        | 23474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=144000, episode_reward=0.70 +/- 16.22\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=144500, episode_reward=-11.43 +/- 25.95\n",
      "Episode length: 200.00 +/- 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -11.4    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 580      |\n",
      "|    fps              | 696      |\n",
      "|    time_elapsed     | 208      |\n",
      "|    total timesteps  | 144900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.325    |\n",
      "|    n_updates        | 23724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=145000, episode_reward=14.84 +/- 7.92\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=145500, episode_reward=23.18 +/- 5.29\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 23.2     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 584      |\n",
      "|    fps              | 695      |\n",
      "|    time_elapsed     | 209      |\n",
      "|    total timesteps  | 145900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.303    |\n",
      "|    n_updates        | 23974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=146000, episode_reward=-485.65 +/- 261.37\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=146500, episode_reward=4.68 +/- 16.62\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 4.68     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 588      |\n",
      "|    fps              | 694      |\n",
      "|    time_elapsed     | 211      |\n",
      "|    total timesteps  | 146900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.392    |\n",
      "|    n_updates        | 24224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=147000, episode_reward=7.12 +/- 4.61\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=147500, episode_reward=17.73 +/- 9.38\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 17.7     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 592      |\n",
      "|    fps              | 694      |\n",
      "|    time_elapsed     | 213      |\n",
      "|    total timesteps  | 147900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.338    |\n",
      "|    n_updates        | 24474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=148000, episode_reward=-166.08 +/- 14.73\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=148500, episode_reward=21.50 +/- 3.58\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 21.5     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 596      |\n",
      "|    fps              | 693      |\n",
      "|    time_elapsed     | 214      |\n",
      "|    total timesteps  | 148900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.44     |\n",
      "|    n_updates        | 24724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=149000, episode_reward=-24.70 +/- 22.35\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=149500, episode_reward=-258.29 +/- 64.20\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -258     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 600      |\n",
      "|    fps              | 692      |\n",
      "|    time_elapsed     | 216      |\n",
      "|    total timesteps  | 149900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.532    |\n",
      "|    n_updates        | 24974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=150000, episode_reward=11.76 +/- 7.95\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=150500, episode_reward=8.27 +/- 21.14\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 8.27     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 604      |\n",
      "|    fps              | 691      |\n",
      "|    time_elapsed     | 218      |\n",
      "|    total timesteps  | 150900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.578    |\n",
      "|    n_updates        | 25224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=151000, episode_reward=9.47 +/- 6.73\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=151500, episode_reward=15.70 +/- 7.05\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 15.7     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 608      |\n",
      "|    fps              | 690      |\n",
      "|    time_elapsed     | 220      |\n",
      "|    total timesteps  | 151900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.588    |\n",
      "|    n_updates        | 25474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=152000, episode_reward=-38.46 +/- 114.64\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=152500, episode_reward=22.01 +/- 2.23\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 22       |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 612      |\n",
      "|    fps              | 688      |\n",
      "|    time_elapsed     | 221      |\n",
      "|    total timesteps  | 152900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.441    |\n",
      "|    n_updates        | 25724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=153000, episode_reward=-70.99 +/- 116.49\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=153500, episode_reward=0.29 +/- 8.97\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 0.294    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 616      |\n",
      "|    fps              | 688      |\n",
      "|    time_elapsed     | 223      |\n",
      "|    total timesteps  | 153900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.759    |\n",
      "|    n_updates        | 25974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=154000, episode_reward=13.29 +/- 16.45\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=154500, episode_reward=9.35 +/- 15.57\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 9.35     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 620      |\n",
      "|    fps              | 687      |\n",
      "|    time_elapsed     | 225      |\n",
      "|    total timesteps  | 154900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.521    |\n",
      "|    n_updates        | 26224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=155000, episode_reward=16.51 +/- 7.58\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=155500, episode_reward=7.46 +/- 10.43\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 7.46     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 624      |\n",
      "|    fps              | 687      |\n",
      "|    time_elapsed     | 226      |\n",
      "|    total timesteps  | 155900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.16     |\n",
      "|    n_updates        | 26474    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=156000, episode_reward=-17.21 +/- 40.06\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=156500, episode_reward=-2.32 +/- 12.19\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -2.32    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 628      |\n",
      "|    fps              | 686      |\n",
      "|    time_elapsed     | 228      |\n",
      "|    total timesteps  | 156900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.3      |\n",
      "|    n_updates        | 26724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=157000, episode_reward=13.87 +/- 12.36\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=157500, episode_reward=15.18 +/- 11.49\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 15.2     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 632      |\n",
      "|    fps              | 686      |\n",
      "|    time_elapsed     | 230      |\n",
      "|    total timesteps  | 157900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.283    |\n",
      "|    n_updates        | 26974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=158000, episode_reward=-13.25 +/- 40.36\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=158500, episode_reward=18.57 +/- 8.32\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 18.6     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 636      |\n",
      "|    fps              | 685      |\n",
      "|    time_elapsed     | 231      |\n",
      "|    total timesteps  | 158900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.475    |\n",
      "|    n_updates        | 27224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=159000, episode_reward=12.26 +/- 17.31\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=159500, episode_reward=16.09 +/- 9.15\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 16.1     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 640      |\n",
      "|    fps              | 684      |\n",
      "|    time_elapsed     | 233      |\n",
      "|    total timesteps  | 159900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.636    |\n",
      "|    n_updates        | 27474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=160000, episode_reward=7.51 +/- 5.69\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=160500, episode_reward=14.29 +/- 7.02\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 14.3     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 644      |\n",
      "|    fps              | 684      |\n",
      "|    time_elapsed     | 235      |\n",
      "|    total timesteps  | 160900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.799    |\n",
      "|    n_updates        | 27724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=161000, episode_reward=24.75 +/- 2.47\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=161500, episode_reward=10.72 +/- 17.41\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 10.7     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 648      |\n",
      "|    fps              | 683      |\n",
      "|    time_elapsed     | 236      |\n",
      "|    total timesteps  | 161900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.561    |\n",
      "|    n_updates        | 27974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=162000, episode_reward=19.15 +/- 3.92\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=162500, episode_reward=17.96 +/- 9.10\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 18       |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 652      |\n",
      "|    fps              | 682      |\n",
      "|    time_elapsed     | 238      |\n",
      "|    total timesteps  | 162900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.572    |\n",
      "|    n_updates        | 28224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=163000, episode_reward=13.31 +/- 11.70\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=163500, episode_reward=7.09 +/- 16.28\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 7.09     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 656      |\n",
      "|    fps              | 681      |\n",
      "|    time_elapsed     | 240      |\n",
      "|    total timesteps  | 163900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.792    |\n",
      "|    n_updates        | 28474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=164000, episode_reward=3.00 +/- 17.19\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=164500, episode_reward=-0.54 +/- 13.07\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -0.536   |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 660      |\n",
      "|    fps              | 680      |\n",
      "|    time_elapsed     | 242      |\n",
      "|    total timesteps  | 164900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.29     |\n",
      "|    n_updates        | 28724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=165000, episode_reward=15.25 +/- 3.95\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=165500, episode_reward=9.69 +/- 15.55\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 9.69     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 664      |\n",
      "|    fps              | 679      |\n",
      "|    time_elapsed     | 244      |\n",
      "|    total timesteps  | 165900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.259    |\n",
      "|    n_updates        | 28974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=166000, episode_reward=6.38 +/- 13.21\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=166500, episode_reward=7.13 +/- 25.49\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 7.13     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 668      |\n",
      "|    fps              | 679      |\n",
      "|    time_elapsed     | 245      |\n",
      "|    total timesteps  | 166900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.219    |\n",
      "|    n_updates        | 29224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=167000, episode_reward=-11.98 +/- 61.15\n",
      "Episode length: 200.00 +/- 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=167500, episode_reward=-37.94 +/- 59.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -37.9    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 672      |\n",
      "|    fps              | 678      |\n",
      "|    time_elapsed     | 247      |\n",
      "|    total timesteps  | 167900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.14     |\n",
      "|    n_updates        | 29474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=168000, episode_reward=3.08 +/- 26.03\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=168500, episode_reward=-0.95 +/- 5.30\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -0.949   |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 676      |\n",
      "|    fps              | 677      |\n",
      "|    time_elapsed     | 249      |\n",
      "|    total timesteps  | 168900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.292    |\n",
      "|    n_updates        | 29724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=169000, episode_reward=-90.48 +/- 118.18\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=169500, episode_reward=-1.81 +/- 26.31\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -1.81    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 680      |\n",
      "|    fps              | 677      |\n",
      "|    time_elapsed     | 250      |\n",
      "|    total timesteps  | 169900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.698    |\n",
      "|    n_updates        | 29974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=170000, episode_reward=-20.78 +/- 78.41\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=170500, episode_reward=13.30 +/- 21.98\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 13.3     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 684      |\n",
      "|    fps              | 676      |\n",
      "|    time_elapsed     | 252      |\n",
      "|    total timesteps  | 170900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.89     |\n",
      "|    n_updates        | 30224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=171000, episode_reward=20.94 +/- 11.03\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=171500, episode_reward=13.16 +/- 15.54\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 13.2     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 688      |\n",
      "|    fps              | 676      |\n",
      "|    time_elapsed     | 254      |\n",
      "|    total timesteps  | 171900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.221    |\n",
      "|    n_updates        | 30474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=172000, episode_reward=-19.27 +/- 32.69\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=172500, episode_reward=21.23 +/- 7.22\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 21.2     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 692      |\n",
      "|    fps              | 675      |\n",
      "|    time_elapsed     | 256      |\n",
      "|    total timesteps  | 172900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.317    |\n",
      "|    n_updates        | 30724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=173000, episode_reward=17.53 +/- 12.78\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=173500, episode_reward=6.11 +/- 15.67\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 6.11     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 696      |\n",
      "|    fps              | 674      |\n",
      "|    time_elapsed     | 257      |\n",
      "|    total timesteps  | 173900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.304    |\n",
      "|    n_updates        | 30974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=174000, episode_reward=-33.82 +/- 31.25\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=174500, episode_reward=-5.16 +/- 53.03\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -5.16    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 700      |\n",
      "|    fps              | 674      |\n",
      "|    time_elapsed     | 259      |\n",
      "|    total timesteps  | 174900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.612    |\n",
      "|    n_updates        | 31224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=175000, episode_reward=12.36 +/- 9.22\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=175500, episode_reward=-2.53 +/- 18.72\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -2.53    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 704      |\n",
      "|    fps              | 673      |\n",
      "|    time_elapsed     | 261      |\n",
      "|    total timesteps  | 175900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.383    |\n",
      "|    n_updates        | 31474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=176000, episode_reward=26.57 +/- 5.59\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=176500, episode_reward=-30.05 +/- 56.64\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -30.1    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 708      |\n",
      "|    fps              | 673      |\n",
      "|    time_elapsed     | 262      |\n",
      "|    total timesteps  | 176900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.243    |\n",
      "|    n_updates        | 31724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=177000, episode_reward=-0.42 +/- 27.80\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=177500, episode_reward=-351.03 +/- 41.03\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -351     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 712      |\n",
      "|    fps              | 672      |\n",
      "|    time_elapsed     | 264      |\n",
      "|    total timesteps  | 177900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.352    |\n",
      "|    n_updates        | 31974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=178000, episode_reward=-26.10 +/- 47.75\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=178500, episode_reward=-11.02 +/- 38.23\n",
      "Episode length: 200.00 +/- 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -11      |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 716      |\n",
      "|    fps              | 671      |\n",
      "|    time_elapsed     | 266      |\n",
      "|    total timesteps  | 178900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.313    |\n",
      "|    n_updates        | 32224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=179000, episode_reward=13.18 +/- 5.97\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=179500, episode_reward=-1.97 +/- 37.88\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -1.97    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 720      |\n",
      "|    fps              | 670      |\n",
      "|    time_elapsed     | 268      |\n",
      "|    total timesteps  | 179900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.286    |\n",
      "|    n_updates        | 32474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=180000, episode_reward=9.96 +/- 24.51\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=180500, episode_reward=-0.63 +/- 30.27\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -0.635   |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 724      |\n",
      "|    fps              | 670      |\n",
      "|    time_elapsed     | 269      |\n",
      "|    total timesteps  | 180900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.967    |\n",
      "|    n_updates        | 32724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=181000, episode_reward=2.92 +/- 10.17\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=181500, episode_reward=23.26 +/- 10.30\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 23.3     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 728      |\n",
      "|    fps              | 669      |\n",
      "|    time_elapsed     | 271      |\n",
      "|    total timesteps  | 181900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.591    |\n",
      "|    n_updates        | 32974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=182000, episode_reward=-12.63 +/- 17.00\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=182500, episode_reward=-6.52 +/- 11.48\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -6.52    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 732      |\n",
      "|    fps              | 669      |\n",
      "|    time_elapsed     | 273      |\n",
      "|    total timesteps  | 182900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.763    |\n",
      "|    n_updates        | 33224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=183000, episode_reward=8.23 +/- 11.09\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=183500, episode_reward=-9.16 +/- 25.66\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -9.16    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 736      |\n",
      "|    fps              | 668      |\n",
      "|    time_elapsed     | 274      |\n",
      "|    total timesteps  | 183900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.324    |\n",
      "|    n_updates        | 33474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=184000, episode_reward=-6.35 +/- 6.21\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=184500, episode_reward=-7.84 +/- 37.21\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -7.84    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 740      |\n",
      "|    fps              | 668      |\n",
      "|    time_elapsed     | 276      |\n",
      "|    total timesteps  | 184900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.614    |\n",
      "|    n_updates        | 33724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=185000, episode_reward=12.70 +/- 5.47\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=185500, episode_reward=8.37 +/- 34.45\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 8.37     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 744      |\n",
      "|    fps              | 668      |\n",
      "|    time_elapsed     | 278      |\n",
      "|    total timesteps  | 185900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.334    |\n",
      "|    n_updates        | 33974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=186000, episode_reward=-9.30 +/- 17.82\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=186500, episode_reward=-3.22 +/- 41.82\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -3.22    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 748      |\n",
      "|    fps              | 667      |\n",
      "|    time_elapsed     | 279      |\n",
      "|    total timesteps  | 186900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.735    |\n",
      "|    n_updates        | 34224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=187000, episode_reward=-10.60 +/- 13.09\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=187500, episode_reward=6.51 +/- 21.96\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 6.51     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 752      |\n",
      "|    fps              | 667      |\n",
      "|    time_elapsed     | 281      |\n",
      "|    total timesteps  | 187900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.314    |\n",
      "|    n_updates        | 34474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=188000, episode_reward=15.02 +/- 11.85\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=188500, episode_reward=7.57 +/- 18.22\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 7.57     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 756      |\n",
      "|    fps              | 666      |\n",
      "|    time_elapsed     | 283      |\n",
      "|    total timesteps  | 188900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.197    |\n",
      "|    n_updates        | 34724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=189000, episode_reward=9.63 +/- 8.83\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=189500, episode_reward=22.18 +/- 7.44\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 22.2     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 760      |\n",
      "|    fps              | 666      |\n",
      "|    time_elapsed     | 284      |\n",
      "|    total timesteps  | 189900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.693    |\n",
      "|    n_updates        | 34974    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=190000, episode_reward=21.81 +/- 7.02\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=190500, episode_reward=26.81 +/- 6.76\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 26.8     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 764      |\n",
      "|    fps              | 666      |\n",
      "|    time_elapsed     | 286      |\n",
      "|    total timesteps  | 190900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.55     |\n",
      "|    n_updates        | 35224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=191000, episode_reward=8.79 +/- 24.21\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=191500, episode_reward=-131.37 +/- 46.83\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -131     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 768      |\n",
      "|    fps              | 665      |\n",
      "|    time_elapsed     | 288      |\n",
      "|    total timesteps  | 191900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.264    |\n",
      "|    n_updates        | 35474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=192000, episode_reward=18.54 +/- 6.85\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=192500, episode_reward=8.57 +/- 11.10\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 8.57     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 772      |\n",
      "|    fps              | 665      |\n",
      "|    time_elapsed     | 289      |\n",
      "|    total timesteps  | 192900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.185    |\n",
      "|    n_updates        | 35724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=193000, episode_reward=-29.31 +/- 27.98\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=193500, episode_reward=2.35 +/- 21.42\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 2.35     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 776      |\n",
      "|    fps              | 665      |\n",
      "|    time_elapsed     | 291      |\n",
      "|    total timesteps  | 193900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.377    |\n",
      "|    n_updates        | 35974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=194000, episode_reward=9.88 +/- 16.42\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=194500, episode_reward=-2.82 +/- 6.03\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -2.82    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 780      |\n",
      "|    fps              | 664      |\n",
      "|    time_elapsed     | 293      |\n",
      "|    total timesteps  | 194900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.263    |\n",
      "|    n_updates        | 36224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=195000, episode_reward=-8.99 +/- 26.37\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=195500, episode_reward=20.72 +/- 5.59\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | 20.7     |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 784      |\n",
      "|    fps              | 664      |\n",
      "|    time_elapsed     | 294      |\n",
      "|    total timesteps  | 195900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.348    |\n",
      "|    n_updates        | 36474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=196000, episode_reward=12.87 +/- 4.96\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=196500, episode_reward=-28.96 +/- 43.79\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -29      |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 788      |\n",
      "|    fps              | 664      |\n",
      "|    time_elapsed     | 296      |\n",
      "|    total timesteps  | 196900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.327    |\n",
      "|    n_updates        | 36724    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=197000, episode_reward=10.10 +/- 14.78\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=197500, episode_reward=-98.96 +/- 14.51\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -99      |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 792      |\n",
      "|    fps              | 663      |\n",
      "|    time_elapsed     | 298      |\n",
      "|    total timesteps  | 197900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.594    |\n",
      "|    n_updates        | 36974    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=198000, episode_reward=15.87 +/- 6.75\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=198500, episode_reward=-14.58 +/- 6.12\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -14.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 796      |\n",
      "|    fps              | 663      |\n",
      "|    time_elapsed     | 299      |\n",
      "|    total timesteps  | 198900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.331    |\n",
      "|    n_updates        | 37224    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=199000, episode_reward=-8.83 +/- 11.94\n",
      "Episode length: 200.00 +/- 0.00\n",
      "Eval num_timesteps=199500, episode_reward=-44.55 +/- 10.92\n",
      "Episode length: 200.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 200      |\n",
      "|    mean_reward      | -44.6    |\n",
      "| rollout/            |          |\n",
      "|    exploration rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 800      |\n",
      "|    fps              | 663      |\n",
      "|    time_elapsed     | 301      |\n",
      "|    total timesteps  | 199900   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.857    |\n",
      "|    n_updates        | 37474    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=200000, episode_reward=12.29 +/- 17.54\n",
      "Episode length: 200.00 +/- 0.00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x7fd3c6035e90>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from stable_baselines3 import PPO, DQN, A2C\n",
    "from stable_baselines3.common.callbacks import CheckpointCallback\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "\n",
    "eval_callback = EvalCallback(env, best_model_save_path='./logs/',\n",
    "                             log_path='./logs/', eval_freq=500,\n",
    "                             deterministic=True, render=False)\n",
    "\n",
    "\n",
    "print(\"Model not found! Starting training...\")\n",
    "policy_kwargs = dict(net_arch=[10,10])\n",
    "model = DQN('MlpPolicy', env, policy_kwargs=policy_kwargs, verbose=1, gamma=1.0, tensorboard_log=\"./logs/\")\n",
    "total_timesteps = 200000\n",
    "model.learn(total_timesteps=total_timesteps,callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO, DQN, A2C\n",
    "\n",
    "# Load best model!\n",
    "model = DQN.load(\"./logs/best_model.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "-PRu_fXD0ENx"
   },
   "outputs": [],
   "source": [
    "# optimal policy agent as per Avellaneda\n",
    "\n",
    "def spread_func(beta, sigma, k):\n",
    "    return lambda T_t: spread(beta, sigma, T_t, k) \n",
    "\n",
    "def r_func(sigma, beta):\n",
    "    return lambda T_t, s, q: r(beta, sigma, T_t, s, q)\n",
    "    \n",
    "class AvellanedaAgent:\n",
    "    def __init__(self, beta, sigma, k):\n",
    "        self.spread_func = spread_func(beta, sigma, k)\n",
    "        self.r_func = r_func(sigma, beta)\n",
    "        \n",
    "    def act(self, observation):\n",
    "        spread = self.spread_func(observation[2])\n",
    "        r_ = self.r_func(observation[2], observation[0], observation[1])\n",
    "        \n",
    "        bid = r_ - spread/2\n",
    "        ask = r_ + spread/2\n",
    "\n",
    "        ds = observation[0] - r_\n",
    "        \n",
    "        #return spread, ds\n",
    "        return ds\n",
    "\n",
    "    def step(self,observation):\n",
    "        return self.act(observation)\n",
    "\n",
    "#agent = AvellanedaAgent(gamma, sigma, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "S7FXF8L50K2V"
   },
   "outputs": [],
   "source": [
    "# symmetrical policy agent as per Avellaneda\n",
    "\n",
    "class SymmetricAgent:\n",
    "    def __init__(self, beta, sigma, k):\n",
    "        self.spread_func = spread_func(beta, sigma, k)\n",
    "        \n",
    "    def act(self, observation):\n",
    "        #spread = self.spread_func(observation[2])\n",
    "        return 0\n",
    "\n",
    "    def step(self,observation):\n",
    "        return self.act(observation)\n",
    "\n",
    "#symmetric_agent = SymmetricAgent(gamma, sigma, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "_GGrBbZWz7pK"
   },
   "outputs": [],
   "source": [
    "def run_env_agent_comp(envs, agent_rl,agent_opt,agent_sym):\n",
    "    \n",
    "    env = envs[0]\n",
    "    \n",
    "    obs = env.reset()\n",
    "    bids_rl = np.zeros(env.n)\n",
    "    asks_rl = np.zeros(env.n)\n",
    "    ss_rl = np.zeros(env.n)\n",
    "    ws_rl = np.zeros(env.n)\n",
    "    qs_rl = np.zeros(env.n)\n",
    "    final = False\n",
    "    i = 0\n",
    "\n",
    "    total_reward_rl = 0.0\n",
    "    while not final:\n",
    "\n",
    "        action_rl = agent_rl.predict(obs,deterministic=True)\n",
    "        ss_rl[i] = obs[0]\n",
    "        qs_rl[i] = obs[1]\n",
    "        \n",
    "        despl = (action_rl[0]-(actions_num-1)/2)*max_abs_dif/(actions_num-1)\n",
    "        ba_spread = spread(env.beta,env.sigma,env.T-env.t,env.k)\n",
    "\n",
    "        bids_rl[i] = ss_rl[i] - despl - ba_spread/2\n",
    "        asks_rl[i] = ss_rl[i] - despl + ba_spread/2\n",
    "\n",
    "        obs, reward, final, w_rl = env.step(action_rl[0])\n",
    "        i += 1\n",
    "        total_reward_rl += reward\n",
    "\n",
    "      \n",
    "    \n",
    "\n",
    "    env = envs[1]\n",
    "    \n",
    "    obs = env.reset()\n",
    "    bids_opt = np.zeros(env.n)\n",
    "    asks_opt = np.zeros(env.n)\n",
    "    ds_opt = np.zeros(env.n)\n",
    "    spread_opt = np.zeros(env.n)\n",
    "    ss_opt = np.zeros(env.n)\n",
    "    ws_opt = np.zeros(env.n)\n",
    "    qs_opt = np.zeros(env.n)\n",
    "    final = False\n",
    "    i = 0\n",
    "\n",
    "    total_reward_opt = 0.0\n",
    "    while not final:\n",
    "        action_opt = agent_opt.step(obs)\n",
    "\n",
    "        ds_opt[i] = action_opt\n",
    "        spread_opt[i] = spread(env.beta,env.sigma,env.T-env.t,env.k)\n",
    "        \n",
    "        ss_opt[i] = obs[0]\n",
    "        qs_opt[i] = obs[1]\n",
    "\n",
    "        bids_opt[i] = ss_opt[i] - ds_opt[i] - spread_opt[i]/2\n",
    "        asks_opt[i] = ss_opt[i] - ds_opt[i] + spread_opt[i]/2\n",
    "\n",
    "        obs, reward, final, w_opt = env.step(action_opt)\n",
    "        total_reward_opt += reward\n",
    "        i += 1\n",
    "\n",
    "    env = envs[2]\n",
    "\n",
    "    obs = env.reset()\n",
    "    bids_sym = np.zeros(env.n)\n",
    "    asks_sym = np.zeros(env.n)\n",
    "    ds_sym = np.zeros(env.n)\n",
    "    spread_sym = np.zeros(env.n)\n",
    "    ss_sym = np.zeros(env.n)\n",
    "    ws_sym = np.zeros(env.n)\n",
    "    qs_sym = np.zeros(env.n)    \n",
    "    final = False\n",
    "    i = 0\n",
    "\n",
    "    total_reward_sym = 0.0\n",
    "    while not final:\n",
    "        action_sym = agent_sym.step(obs)\n",
    "\n",
    "        ds_sym[i] = action_sym\n",
    "        spread_sym[i] = spread(env.beta,env.sigma,env.T-env.t,env.k)\n",
    "        \n",
    "        ss_sym[i] = obs[0]\n",
    "        qs_sym[i] = obs[1]\n",
    "\n",
    "        bids_sym[i] = ss_sym[i] - ds_sym[i] - spread_sym[i]/2\n",
    "        asks_sym[i] = ss_sym[i] - ds_sym[i] + spread_sym[i]/2\n",
    "        \n",
    "        obs, reward, final, w_sym = env.step(action_sym)\n",
    "        i += 1\n",
    "        total_reward_sym += reward\n",
    "\n",
    "        \n",
    "    return w_rl['w'], w_opt['w'], w_sym['w'],total_reward_rl,total_reward_opt,total_reward_sym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "pgV6nOhgAupx"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0%\n",
      "1.0%\n",
      "2.0%\n",
      "3.0%\n",
      "4.0%\n",
      "5.0%\n",
      "6.0%\n",
      "7.0%\n",
      "8.0%\n",
      "9.0%\n",
      "10.0%\n",
      "11.0%\n",
      "12.0%\n",
      "13.0%\n",
      "14.0%\n",
      "15.0%\n",
      "16.0%\n",
      "17.0%\n",
      "18.0%\n",
      "19.0%\n",
      "20.0%\n",
      "21.0%\n",
      "22.0%\n",
      "23.0%\n",
      "24.0%\n",
      "25.0%\n",
      "26.0%\n",
      "27.0%\n",
      "28.0%\n",
      "29.0%\n",
      "30.0%\n",
      "31.0%\n",
      "32.0%\n",
      "33.0%\n",
      "34.0%\n",
      "35.0%\n",
      "36.0%\n",
      "37.0%\n",
      "38.0%\n",
      "39.0%\n",
      "40.0%\n",
      "41.0%\n",
      "42.0%\n",
      "43.0%\n",
      "44.0%\n",
      "45.0%\n",
      "46.0%\n",
      "47.0%\n",
      "48.0%\n",
      "49.0%\n",
      "50.0%\n",
      "51.0%\n",
      "52.0%\n",
      "53.0%\n",
      "54.0%\n",
      "55.0%\n",
      "56.0%\n",
      "57.0%\n",
      "58.0%\n",
      "59.0%\n",
      "60.0%\n",
      "61.0%\n",
      "62.0%\n",
      "63.0%\n",
      "64.0%\n",
      "65.0%\n",
      "66.0%\n",
      "67.0%\n",
      "68.0%\n",
      "69.0%\n",
      "70.0%\n",
      "71.0%\n",
      "72.0%\n",
      "73.0%\n",
      "74.0%\n",
      "75.0%\n",
      "76.0%\n",
      "77.0%\n",
      "78.0%\n",
      "79.0%\n",
      "80.0%\n",
      "81.0%\n",
      "82.0%\n",
      "83.0%\n",
      "84.0%\n",
      "85.0%\n",
      "86.0%\n",
      "87.0%\n",
      "88.0%\n",
      "89.0%\n",
      "90.0%\n",
      "91.0%\n",
      "92.0%\n",
      "93.0%\n",
      "94.0%\n",
      "95.0%\n",
      "96.0%\n",
      "97.0%\n",
      "98.0%\n",
      "99.0%\n"
     ]
    }
   ],
   "source": [
    "number_of_sims = 1000\n",
    "\n",
    "n = int(T/dt)\n",
    "ws_rl = np.zeros(number_of_sims)\n",
    "ws_opt = np.zeros(number_of_sims)\n",
    "ws_sym = np.zeros(number_of_sims)\n",
    "tr_rl = np.zeros(number_of_sims)\n",
    "tr_opt = np.zeros(number_of_sims)\n",
    "tr_sym = np.zeros(number_of_sims)\n",
    "\n",
    "envs = [AvellanedaEnv(s0, T, dt, sigma, beta, k, A, kappa),AvellanedaEnv(s0, T, dt, sigma, beta, k, A,kappa, seed=0, is_discrete=False),AvellanedaEnv(s0, T, dt, sigma, beta, k, A,kappa, seed=0, is_discrete=False)]\n",
    "for i in range(number_of_sims):\n",
    "    if i%10 == 0:\n",
    "        print(str(i/10) + \"%\")\n",
    "    ws_rl[i], ws_opt[i], ws_sym[i], tr_rl[i], tr_opt[i], tr_sym[i] = run_env_agent_comp(envs, model,AvellanedaAgent(beta, sigma, k),SymmetricAgent(beta, sigma, k))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Byg9c4wIC22a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Accumulated wealth histogram')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Figure 2 (p. 222)\n",
    "fig=plt.figure(figsize=(20,8), dpi= 100, facecolor='w', edgecolor='k')\n",
    "plt.hist([ws_opt,ws_rl], bins=30,edgecolor='black', label=['Optimum','RL Discrete'])\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlabel(\"Wealth\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Accumulated wealth histogram\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "5QZDlL5dN3DX"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Accumulated wealth histogram')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 2000x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig=plt.figure(figsize=(20,8), dpi= 100, facecolor='w', edgecolor='k')\n",
    "plt.hist([tr_opt,tr_sym,tr_rl], bins=30,edgecolor='black', label=['Optimum', 'Symmetric','RL Discrete'])\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlabel(\"Total Reward\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Accumulated wealth histogram\")\n",
    "fig=plt.figure(figsize=(20,8), dpi= 100, facecolor='w', edgecolor='k')\n",
    "plt.hist([tr_opt,tr_rl], bins=30,edgecolor='black', label=['Optimum','RL Discrete'])\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlabel(\"Total Reward\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.title(\"Accumulated wealth histogram\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "LUl_ON_bDS1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimo:\n",
      "47.79761695404737\n",
      "6.099978058681764\n",
      "7.83570309503321\n",
      "Simetrico:\n",
      "57.676984427753716\n",
      "11.86348257940186\n",
      "4.8617245435076715\n",
      "RL:\n",
      "53.47222338958933\n",
      "6.676254695245594\n",
      "8.009314478021468\n",
      "\n",
      "Optimum utility function value: \t-2.6283680458275698e-09\n",
      "Symmetric utility function value: \t-4.34167929750502e-06\n",
      "RL utility function value: \t\t-2.207418591662263e-10\n"
     ]
    }
   ],
   "source": [
    "print(\"Optimo:\")\n",
    "print(np.mean(ws_opt))\n",
    "print(np.std(ws_opt))\n",
    "print(np.mean(ws_opt)/np.std(ws_opt))\n",
    "print(\"Simetrico:\")\n",
    "print(np.mean(ws_sym))\n",
    "print(np.std(ws_sym))\n",
    "print(np.mean(ws_sym)/np.std(ws_sym))\n",
    "print(\"RL:\")\n",
    "print(np.mean(ws_rl))\n",
    "print(np.std(ws_rl))\n",
    "print(np.mean(ws_rl)/np.std(ws_rl))\n",
    "\n",
    "print()\n",
    "\n",
    "utility_avellaneda = np.mean(-np.exp(-beta*ws_opt))\n",
    "utility_rl = np.mean(-np.exp(-beta*ws_rl))\n",
    "\n",
    "print(\"Optimum utility function value: \\t{}\".format(utility_avellaneda))\n",
    "print(\"Symmetric utility function value: \\t{}\".format(np.mean(-np.exp(-beta*ws_sym))))\n",
    "print(\"RL utility function value: \\t\\t{}\".format(utility_rl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "UJqDqpVIeGti"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimo:\n",
      "22.46050972128678\n",
      "3.4646472377108304\n",
      "6.482769580930536\n",
      "Simetrico:\n",
      "-7.172303914746957\n",
      "42.1491264960118\n",
      "-0.1701649479123988\n",
      "RL:\n",
      "29.04973560849187\n",
      "4.686901148377606\n",
      "6.19806876416576\n"
     ]
    }
   ],
   "source": [
    "print(\"Optimo:\")\n",
    "print(np.mean(tr_opt))\n",
    "print(np.std(tr_opt))\n",
    "print(np.mean(tr_opt)/np.std(tr_opt))\n",
    "\n",
    "print(\"Simetrico:\")\n",
    "print(np.mean(tr_sym))\n",
    "print(np.std(tr_sym))\n",
    "print(np.mean(tr_sym)/np.std(tr_sym))\n",
    "\n",
    "print(\"RL:\")\n",
    "print(np.mean(tr_rl))\n",
    "print(np.std(tr_rl))\n",
    "print(np.mean(tr_rl)/np.std(tr_rl))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('out_ws.csv', 'w') as f: \n",
    "      \n",
    "    # using csv.writer method from CSV package \n",
    "    write = csv.writer(f)\n",
    "    write.writerow(['DQN', 'OPT','SYM']) \n",
    "    for index, _ in enumerate(ws_rl):\n",
    "        write.writerow([ws_rl[index], ws_opt[index],ws_sym[index]]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('out_tr.csv', 'w') as f: \n",
    "      \n",
    "    # using csv.writer method from CSV package \n",
    "    write = csv.writer(f)\n",
    "    write.writerow(['DQN', 'OPT','SYM']) \n",
    "    for index, _ in enumerate(tr_rl):\n",
    "        write.writerow([tr_rl[index], tr_opt[index],tr_sym[index]]) "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "sim_corregida.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
